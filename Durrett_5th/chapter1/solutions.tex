\documentclass[a4paper]{article}

\usepackage{../../header}

\newcommand{\thechapter}{1}

\renewcommand{\theexercise}{\thechapter.\arabic{section}.\arabic{exercise}}

\renewcommand{\thetheorem}{\thechapter.\arabic{section}.\arabic{theorem}}

\title{Durrett 5th Chapter{\thechapter} Solutions}
\author{htao}

\begin{document}
\maketitle

\section{Probability Spaces}
\begin{exercise}
    Let $\Omega=\RR$, $\mathcal{F}=$all subsets so that $A$ or $A^c$ is countable, $P(A)=0$ in the first case and $= 1$ in the second.
    Show that $(\Omega,\mathcal{F}, P)$ is a probability space.
\end{exercise}
\begin{solution}
    We firstly prove $\mathcal{F}$ is a $\sigma$-field. Let $\Set{A_i} \subset \mathcal{F}$. It is obvious $A_i^c\in \mathcal{F}$.
    If $A_i$ or $A_j$ is countable, then $A_i\cap A_j$ is countable and hence contained in $\mathcal{F}$. Otherwise $A_i^c$ and $A_j^c$ is countable,
    and then $(A_i\cap A_j)^c=A_i^c\cup A_j^c$ is countable, which concludes $A_i\cap A_j$ is always in $\mathcal{F}$. Finally, $\bigcup_{i\in\NN} A_i$ is countable if every $A_i$ is countable, Otherwise $(\bigcup_{i\in\NN} A_i)^c = \bigcap_{i\in\NN} A_i^c\subset A_0^c$ is countable since $A_0^c$ is countable, which concludes $\bigcup_{i\in\NN} A_i \in \mathcal{F}$. Hence $\mathcal{F}$ is a $\sigma$-field.

    $P$ is well-defined since $A\cup A^c=\Omega=\RR$ is uncountable, whence $A$ and $A^c$ can not be countable simutaneously. Finally we prove $P$ is a probability. It is trivial $P(\Omega)=1$. For disjoint sets, $A_i \in \mathcal{F}$, if $A_i$ are all countable, $P(A_i)=0$
    and then $+_{i=1}^\infty A_i$ is countable, $P(+_{i=1}^\infty A_i)=0=\sum_{i=1}^{\infty} P(A_i)$. Otherwise $A_k^c$ is countable, and then $A_i\subset A_k^c, i\neq k$ is countable since they are disjoint. And  $(+_{i\in\NN} A_i)^c = \bigcap_{i\in\NN} A_i^c\subset A_k^c$ is countable. So $P(+_{i=1}^\infty A_i)=\sum_{i=1}^{\infty} P(A_i)=1=P(+_{i\in\NN} A_i)$, when $P$ is a probability.
\end{solution}


\begin{exercise}
    Recall the definition of $S_d$ from Example 1.1.5. Show that $\sigma(S_d) = \mathcal{R}^d$, the Borel subsets of $\RR^d$.
\end{exercise}
\begin{proof}
    The definition of $S_d$ is
    \[
        S_d = \Dset{\prod_{i=1}^d (a_i,b_i]}{-\infty\le a,b\le +\infty}
    \]
    It is obvious $S_d\subset\mathcal{R}^d$ whence $\sigma(S_d)\subset\mathcal{R}^d$.Then for any $\prod_{i=1}^d (a_i,b_i)$,
    \[
        \bigcup_{n=1}^\infty (\prod_{i=1}^d (a_i,b_i-\frac{1}{n}]) = \prod_{i=1}^d (a_i,b_i)
    \]
    The desired result follows from $\mathcal{R}^d$ is the $\sigma$-field of the right.
\end{proof}

\begin{exercise}
    A $\sigma$-field $\mathcal{F}$ is said to be countably generated if there is a countable collection $\mathcal{C} \subset \mathcal{F}$
    so that $\sigma(C) = F$. Show that $\mathcal{R}^d$ is countably generated.
\end{exercise}

\begin{solution}
    Let
    \[
        \mathcal{U} = \Dset{\prod_{i=1}^{d} (a_i, b_i)}{a_i,b_i\in \QQ}
    \]
    Then $\mathcal{U}$ is countable since $\abs{\mathcal{U}}\le \abs{\QQ^{2d}}$. And $\sigma(\mathcal{U})=\mathcal{R}^d$ since for each  $\prod_{i=1}^d (a_i,b_i)$, there are $a_{i,k}, b_{i,k}\in \QQ$ such that $a_{i,k}\to a_i+0$ and $b_{i,k}\to b_i-0$ as $k\to \infty$ and then
    \[
        \bigcup_{k=1}^\infty (\prod_{i=1}^d (a_{i,k},b_{i,k})) = \prod_{i=1}^d (a_i,b_i)
    \]
\end{solution}

\begin{exercise}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item Show that if $\mathcal{F}_1 \subset  \mathcal{F}_2 \subset \dots$ are $\sigma$-algebras, then $\bigcup_i\mathcal{F}_i$ is an algebra.
        \item Give an example to show that  $\bigcup_i\mathcal{F}_i$ need not be a $\sigma$-algebra.
    \end{enumerate}
\end{exercise}


\begin{solution}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item For $A,B\in  \bigcup_i\mathcal{F}_i$, there is $F_k$ such that $A,B\in \mathcal{F}_k$, and then $A\cup B,A^c \in \mathcal{F}_k\subset  \bigcup_i\mathcal{F}_i$, whence  $\bigcup_i\mathcal{F}_i$ is an algebra.
        \item Let $\Omega=\RR^\NN$ and $F_i=\Dset{U\times \RR^\NN}{U\in \mathcal{R}^i}$. While $[0,1]^i\times\RR^\NN\in\mathcal{F}_i$, but
              \[
                  [0,1]^\NN = \bigcap_{i=1}^\infty [0,1]^i\times\RR^\NN \notin  \bigcup_i\mathcal{F}_i
              \]

    \end{enumerate}
\end{solution}

\begin{exercise}
    A set $A \subset \Set{1, 2,\dots}$ is said to have asymptotic density  $\theta$ if
    \[
        \lim_{n\to\infty} \frac{\abs{A \cap \Set{1, 2, . . . , n}}}{n} = \theta
    \]
    Let $\mathcal{A}$ be the collection of sets for which the asymptotic density exists.
    Is $\mathcal{A}$ a $\sigma$-algebra? an algebra?
\end{exercise}

\begin{solution}
    $\mathcal{A}$ is not an algebra. The counterexample is shown in the following.

    Let $A=\Set{2n}$ and $B=\Set{b_n}, b_n\in\Set{2n-1, 2n}$. Then $A,B$ have asymptotic densities $\flatfrac{1}{2}$.
    And then let
    \[
        S_n = \abs{A\cap B \cap \Set{1,2,\dots,2n}} = \sum_{k=1}^{n} \abs{A\cap B \cap \Set{2k-1,2k}} = \sum_{k=1}^{n} c_k
    \]
    where
    \[
        c_k=\begin{cases}
            0 & b_k=2k-1 \\
            1 & b_k=2k
        \end{cases}
    \]
    It means for any series $\Set{x_i}, x_i\in\Set{0,1}$, there exists $B$ with asymptotic density $\flatfrac{1}{2}$ such that $c_i=x_i$.
    Hence let
    \[
        c_i = \begin{cases}
            0 & i=1                         \\
            1 & i = [3^{2k}+1  ,3^{2k+1}]   \\
            0 & i = [3^{2k+1}+1  ,3^{2k+2}]
        \end{cases}
        \qquad        k\in\NN
    \]


    Then for $n=3^{2k+1}$, $S_n \ge \sum_{3^{2k}+1}^{3^{2k+1}} c_i = 2\times 3^{2k}= \frac{2}{3}n$  and for $n=3^{2k+2}$, $S_n= S_{3^{2k+1}}\le 3^{2k+1}= \frac{1}{3}n$.
    So $L_n:=\flatfrac{ \abs{A\cap B \cap \Set{1,2,\dots,n}}}{n}\ge \flatfrac{1}{3}$ when $n=2\times3^{2k+1}$ and $L_n\le \frac{1}{6}$ when
    $n=2\times 3^{2k+2}$, whence $L_n$ diverges and then $\mathcal{A}$ is not an algebra.

    \textit{(The motivation here is for $0<\flatfrac{S_n}{n}$<1 we could add a lot of 1's to making $\flatfrac{S_{n+m}}{n+m}>1-\epsilon$ for any epsilon;
        and a lot of 0's to making $\flatfrac{S_{n+m}}{n+m}<\epsilon$ for any $\epsilon>0$. Thus the quotinet could fluctuates and then not converge.)}
\end{solution}

\section{Distributions}
\begin{exercise}
    Suppose $X$ and $Y$ are random variables on $(\Omega,\mathcal{F}, P)$ and let $A \in \mathcal{F}$.
    Show that if we let $Z(\omega) = X(\omega)$ for $\omega \in A$ and $Z(\omega) = Y (\omega)$ for
    $ \omega\in A^c$, then Z is a random variable.
\end{exercise}
\begin{solution}
    $Z = X\cdot1_A+Y\cdot1_{A^c}$ is a random variable.
\end{solution}

\begin{exercise}
    Let $\chi$ have the standard normal distribution. Use Theorem 1.2.6 to get upper and lower bounds on $P(\chi\le 4)$.
\end{exercise}
\begin{solution}
    Theorem 1.2.6 is listed at \cref{thm: original 1.2.6}.
    \[
        P(\chi\le 4) = \int_{-\infty}^{4} e^{\flatfrac{x^2}{2}}\dif x = 1 - \int_{x}^{+\infty} e^{\flatfrac{x^2}{2}}\dif x
    \]
    Applying the theorem gives
    \[
        (\frac{1}{4}-\frac{1}{64})e^{-8} \le \int_{x}^{+\infty} e^{\flatfrac{x^2}{2}}\dif x \le \frac{1}{4}e^{-8}
    \]
    So
    \[
        1-\frac{1}{4}e^{-8} \le P(\chi\le 4) \le 1-\frac{15e^{-8}}{64}
    \]
\end{solution}
\begin{exercise}
    Show that a distribution function has at most countably many discontinuities.
\end{exercise}
\begin{solution}
    This is a common conlusion in real analysis. Let $f$ be a distribution function, then $f$ is monotonic increasing.
    Let $P$ be the set of discontinuities.
    For each $x_0\in P$, $f_-(x_0)$, $f_+(x_0)$ exist and $f_-(x_0)$ < $f_-(x_0)$.
    Then letting
    \[
        \mathcal{I}:= \Dset{\left(f_-(a), f_+(a)\right)}{a\in P}
    \]
    and
    \begin{align*}
        \varphi: & P\to \mathcal{I}                  \\
                 & x_0 \mapsto (f_-(x_0) , f_-(x_0))
    \end{align*}

    intervals in $\mathcal{I}$ are disjoint and $\varphi$ is bijective and in since for any $x<y$,
    \[
        f_-(x) < f_+(x)\le f_-(y) < f_+(y)
    \]
    The desired result follows from $\mathcal{I}$, a collection of disjoint open intervals in $\RR$, is at most countable since $\exists q_{a,b}\in \QQ$, $q_{a,b}\in(a,b)\in\mathcal{I}$,
    and then $(a,b)\mapsto q_{a,b}$ is an injection to $\QQ$.
\end{solution}

\begin{exercise}
    Show that if $F(x) = P(X \le x)$ is continuous then $Y = F(X)$ has
    a uniform distribution on $(0,1)$, that is, if $y \in [0, 1]$, $P(Y \le y) = y$.
\end{exercise}
\begin{proof}
    We have $0 \ge F(x)\le 1$, so $P(Y\le y)=0$ for $y<0$ and $P(Y\le y)=1$ for $y>1$. For $y\in [0,1]$,
    \[
        P(Y\le y) = P(F(X) \le y) = P(X\in \Dset{h}{F(h)\le y})
    \]
    Let $h_0 = \sup \Dset{h}{F(h)\le y}$. Since $F$ is increasing and continuous,  $F(h_0) = y$. Therefore
    \[
        P(Y\le y) = P(X\le h_0) = F(h_0)=y
    \]
\end{proof}
\begin{exercise}
    \label{exc: 1.2.5}
    Suppose $X$ has continuous density $f$, $P( \alpha \le  X\le \beta) = 1$ and
    g is a function that is strictly increasing and differentiable on $(\alpha,\beta)$.
    Then $g(X)$ has density
    \[
        \rho (y)=\begin{cases}

            \frac{f(g^{-1}(y))}{g'(g^{-1}(y))} & y \in (g(\alpha), g(\beta)) \\
            0                                  & otherwise
        \end{cases}
    \]
    When $g(x) = ax + b$ with $a > 0$, $g^{-1}(y) = \frac{y - b}{a}$ so the answer is $\frac{1}{a}f(\frac{y - b}{a}).$
\end{exercise}
\begin{proof}
    Since $g$ is strictly increasing, $g$ is invertible and
    therefore
    \[
        P(g(X)\le y) = P(X\le g^{-1}(y))=
        \begin{cases}
            0                                    & y\in (-\infty, g(\alpha)] \\
            \int_{\alpha}^{g^{-1}(y)} f(x)\dif x & y\in(g(\alpha), g(\beta)) \\
            1                                    & y\in [g(\beta), +\infty)
        \end{cases}
    \]
    We could direct differentiate the right or replace $x=g^{-1}(u)$, and then
    \[
        \int_{\alpha}^{g^{-1}(y)} f(x)\dif x = \int_{g(\alpha)}^{y} f(g^{-1}(u))\dif g^{-1}(u) = \int_{g(\alpha)}^{y} \frac{f(g^{-1}(u))}{g'(g^{-1}(u))} \dif u
    \]
\end{proof}
\begin{exercise}
    Suppose $X$ has a normal distribution. Use the previous exercise to compute the density of $\exp(X)$.
    (The answer is called the lognormal distribution.)
\end{exercise}
\begin{proof}
    By \cref{exc: 1.2.5}, letting $f(x)=\frac{1}{\sqrt{2\pi}}e^{\flatfrac{-x^2}{2}}$, the distribution is
    \[
        g(x) = \begin{cases}
            \frac{f(\ln x)}{\exp(\ln(x))} = \frac{1}{x\sqrt{2\pi}}e^{-\frac{(\ln x)^2}{2}} & x\in (0,+\infty)  \\
            0                                                                              & x\in (-\infty, 0]
        \end{cases}
    \]
\end{proof}

\begin{exercise}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item Suppose $X$ has density function $f$. Compute the distribution function of $X^2$ and then differentiate to find its density function.
        \item Work out the answer when X has a standard normal distribution to find the density of the chi-square distribution.
    \end{enumerate}
\end{exercise}
\begin{proof}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item Since
              \[
                  P(X^2\le y) =
                  \begin{cases}
                      0                                        & y\in (-\infty, 0] \\
                      P\left(-\sqrt{y}\le X\le \sqrt{y}\right) & y\in (0,+\infty)
                  \end{cases}
              \]
              and replacing $x=\sqrt{u}$,
              \begin{align*}
                  P\left(-\sqrt{y}\le X\le \sqrt{y}\right) & = \int_{-\sqrt{y}}^{\sqrt{y}} f(x)\dif x = \int_{0}^{\sqrt{y}} (f(x)+f(-x))\dif x                                     \\
                                                           & =\int_0^{y} (f(\sqrt{u})+ f(-\sqrt{u})) \dif \sqrt{u} = \int_0^{y} \frac{f(\sqrt{u})+ f(-\sqrt{u})}{2\sqrt{u}} \dif u
              \end{align*}
              $X^2$ has a density function,
              \[
                  g(y)=\begin{cases}
                      0                                        & y\in (-\infty, 0] \\
                      \frac{f(\sqrt{y})+ f(-\sqrt{y})}{2\sqrt{y}} & y\in (0,+\infty)
                  \end{cases}
              \]

              (\textit{We can not directly differentiate the definite intergrals with variable limits, since it may be indifferentiable when $f$ is discontinuous})
        \item Applying above conclusion, the density is 
        \[
                  g(y)=\begin{cases}
                      0                                        & y\in (-\infty, 0] \\
                      \frac{e^{\flatfrac{-y}{2}}}{\sqrt{2\pi y}} & y\in (0,+\infty)
                  \end{cases}
        \]
    \end{enumerate}
\end{proof}
\appendix
\section{Some related theorem details}

\stepcounter{section}

\subsection*{2\quad Distributions}

\begin{theorem}
    \label{thm: asymptotic expansion of the Gaussian tail probability}
    For $x>0$, leting
    \[
        I(x) := \int_{x}^{+\infty} e^{-\flatfrac{y^2}{2}}
    \]
    then for $n\in \NN$,
    \[
        I(x)= I_n(x) + R_n(x)
    \]
    where
    \begin{align*}
        I_n(x)
        : & = e^{-x^2/2} \left( \frac{1}{x} - \frac{1}{x^3} + \frac{3}{x^5} - \frac{15}{x^7} + \cdots + (-1)^{n-1} \frac{(2n-3)!!}{x^{2n-1}} \right) \\
          & = e^{-x^2/2} \sum_{k=1}^n (-1)^{k-1} \frac{(2k-3)!!}{x^{2k-1}}
    \end{align*}
    and
    \[
        R_n(x) := (-1)^n (2n-1)!! \int_x^{+\infty} \frac{e^{-y^2/2}}{y^{2n}}  \dif y
    \]
    by setting $(-1)!!=1$.
    Then $I_{2k}(x)<I(x) <I_{2k+1}(x)$ for any $k\in\NN$.
\end{theorem}
\begin{proof}
    Use induction on $n$. It's trivial for $n=0$ and assume it holds for $n=k$.
    Since
    \begin{align*}
        \frac{R_k(x)}{(-1)^k (2k-1)!!} & =  \int_x^\infty \frac{e^{-y^2/2}}{y^{2k}}  \dif y  =  \int_x^{+\infty} -\frac{1}{y^{2k+1}}  \dif e^{-y^2/2}       \\
                                       & = \left.\frac{-e^{-y^2/2}}{y^{2k+1}}\right|^{+\infty}_x - \int_x^{+\infty} \frac{(2k+1)e^{-y^2/2}}{y^{2k+2}}\dif y \\\
                                       & = \frac{-e^{-x^2/2}}{x^{2k+1}} -(2k+1) \int_x^{+\infty} \frac{e^{-y^2/2}}{y^{2k+2}}\dif y                          \\
                                       & = \frac{-e^{-x^2/2}}{x^{2k+1}} + \frac{R_{k+1}(x)}{(-1)^k (2k-1)!!}
    \end{align*}

    then
    \begin{align*}
        I & = I_k + R_k = I_k + (-1)^k (2k-1)!!\frac{-e^{-x^2/2}}{x^{2k+1}} + R_{k+1}(x) \\
          & = I_{k+1}(x) + R_{k+1}(x)                                                    \\
    \end{align*}
    The last conclusion is obvious since $R_{2k}>0$ and $R_{2k+1}<0$ for any $k\in \NN$.
\end{proof}
\begin{proof}

\end{proof}

\begin{theorem}[Theorem 1.2.6 in PTE5th]
    \label{thm: original 1.2.6}
    For $x> 0$,
    \[
        (x^{-1}-x^{-3})e^{-\flatfrac{x^2}{2}} \le \int_{x}^{+\infty} e^{-\flatfrac{y^2}{2}} \dif y \le  x^{-1}e^{-\flatfrac{x^2}{2}}
    \]
\end{theorem}
\begin{proof}
    We give a stronger conclusion and a more general proof in  \cref{thm: asymptotic expansion of the Gaussian tail probability}. The desired result directly follows from the cases of $n=1$ and $n=2$.
\end{proof}

\end{document}