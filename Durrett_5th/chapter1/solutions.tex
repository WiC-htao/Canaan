\documentclass[a4paper]{article}

\usepackage{../../header}

\newcommand{\thechapter}{1}

\renewcommand{\theexercise}{\thechapter.\arabic{section}.\arabic{exercise}}

\renewcommand{\thetheorem}{\thechapter.\arabic{section}.\arabic{theorem}}

\title{Durrett 5th Chapter{\thechapter} Solutions}
\author{htao}

\begin{document}
\maketitle

\section{Probability Spaces}
\begin{exercise}
    Let $\Omega=\RR$, $\mathcal{F}=$all subsets so that $A$ or $A^c$ is countable, $P(A)=0$ in the first case and $= 1$ in the second.
    Show that $(\Omega,\mathcal{F}, P)$ is a probability space.
\end{exercise}
\begin{solution}
    We firstly prove $\mathcal{F}$ is a $\sigma$-field. Let $\Set{A_i} \subset \mathcal{F}$. It is obvious $A_i^c\in \mathcal{F}$.
    If $A_i$ or $A_j$ is countable, then $A_i\cap A_j$ is countable and hence contained in $\mathcal{F}$. Otherwise $A_i^c$ and $A_j^c$ is countable,
    and then $(A_i\cap A_j)^c=A_i^c\cup A_j^c$ is countable, which concludes $A_i\cap A_j$ is always in $\mathcal{F}$. Finally, $\bigcup_{i\in\NN} A_i$ is countable if every $A_i$ is countable, Otherwise $(\bigcup_{i\in\NN} A_i)^c = \bigcap_{i\in\NN} A_i^c\subset A_0^c$ is countable since $A_0^c$ is countable, which concludes $\bigcup_{i\in\NN} A_i \in \mathcal{F}$. Hence $\mathcal{F}$ is a $\sigma$-field.

    $P$ is well-defined since $A\cup A^c=\Omega=\RR$ is uncountable, whence $A$ and $A^c$ can not be countable simutaneously. Finally we prove $P$ is a probability. It is trivial $P(\Omega)=1$. For disjoint sets, $A_i \in \mathcal{F}$, if $A_i$ are all countable, $P(A_i)=0$
    and then $+_{i=1}^\infty A_i$ is countable, $P(+_{i=1}^\infty A_i)=0=\sum_{i=1}^{\infty} P(A_i)$. Otherwise $A_k^c$ is countable, and then $A_i\subset A_k^c, i\neq k$ is countable since they are disjoint. And  $(+_{i\in\NN} A_i)^c = \bigcap_{i\in\NN} A_i^c\subset A_k^c$ is countable. So $P(+_{i=1}^\infty A_i)=\sum_{i=1}^{\infty} P(A_i)=1=P(+_{i\in\NN} A_i)$, when $P$ is a probability.
\end{solution}


\begin{exercise}
    Recall the definition of $S_d$ from Example 1.1.5. Show that $\sigma(S_d) = \mathcal{R}^d$, the Borel subsets of $\RR^d$.
\end{exercise}
\begin{proof}
    The definition of $S_d$ is
    \[
        S_d = \Dset{\prod_{i=1}^d (a_i,b_i]}{-\infty\le a,b\le +\infty}
    \]
    It is obvious $S_d\subset\mathcal{R}^d$ whence $\sigma(S_d)\subset\mathcal{R}^d$.Then for any $\prod_{i=1}^d (a_i,b_i)$,
    \[
        \bigcup_{n=1}^\infty (\prod_{i=1}^d (a_i,b_i-\frac{1}{n}]) = \prod_{i=1}^d (a_i,b_i)
    \]
    The desired result follows from $\mathcal{R}^d$ is the $\sigma$-field of the right.
\end{proof}

\begin{exercise}
    A $\sigma$-field $\mathcal{F}$ is said to be countably generated if there is a countable collection $\mathcal{C} \subset \mathcal{F}$
    so that $\sigma(C) = F$. Show that $\mathcal{R}^d$ is countably generated.
\end{exercise}

\begin{solution}
    Let
    \[
        \mathcal{U} = \Dset{\prod_{i=1}^{d} (a_i, b_i)}{a_i,b_i\in \QQ}
    \]
    Then $\mathcal{U}$ is countable since $\abs{\mathcal{U}}\le \abs{\QQ^{2d}}$. And $\sigma(\mathcal{U})=\mathcal{R}^d$ since for each  $\prod_{i=1}^d (a_i,b_i)$, there are $a_{i,k}, b_{i,k}\in \QQ$ such that $a_{i,k}\to a_i+0$ and $b_{i,k}\to b_i-0$ as $k\to \infty$ and then
    \[
        \bigcup_{k=1}^\infty (\prod_{i=1}^d (a_{i,k},b_{i,k})) = \prod_{i=1}^d (a_i,b_i)
    \]
\end{solution}

\begin{exercise}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item Show that if $\mathcal{F}_1 \subset  \mathcal{F}_2 \subset \dots$ are $\sigma$-algebras, then $\bigcup_i\mathcal{F}_i$ is an algebra.
        \item Give an example to show that  $\bigcup_i\mathcal{F}_i$ need not be a $\sigma$-algebra.
    \end{enumerate}
\end{exercise}


\begin{solution}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item For $A,B\in  \bigcup_i\mathcal{F}_i$, there is $F_k$ such that $A,B\in \mathcal{F}_k$, and then $A\cup B,A^c \in \mathcal{F}_k\subset  \bigcup_i\mathcal{F}_i$, whence  $\bigcup_i\mathcal{F}_i$ is an algebra.
        \item Let $\Omega=\RR^\NN$ and $F_i=\Dset{U\times \RR^\NN}{U\in \mathcal{R}^i}$. While $[0,1]^i\times\RR^\NN\in\mathcal{F}_i$, but
              \[
                  [0,1]^\NN = \bigcap_{i=1}^\infty [0,1]^i\times\RR^\NN \notin  \bigcup_i\mathcal{F}_i
              \]

    \end{enumerate}
\end{solution}

\begin{exercise}
    A set $A \subset \Set{1, 2,\dots}$ is said to have asymptotic density  $\theta$ if
    \[
        \lim_{n\to\infty} \frac{\abs{A \cap \Set{1, 2, . . . , n}}}{n} = \theta
    \]
    Let $\mathcal{A}$ be the collection of sets for which the asymptotic density exists.
    Is $\mathcal{A}$ a $\sigma$-algebra? an algebra?
\end{exercise}

\begin{solution}
    $\mathcal{A}$ is not an algebra. The counterexample is shown in the following.

    Let $A=\Set{2n}$ and $B=\Set{b_n}, b_n\in\Set{2n-1, 2n}$. Then $A,B$ have asymptotic densities $\flatfrac{1}{2}$.
    And then let
    \[
        S_n = \abs{A\cap B \cap \Set{1,2,\dots,2n}} = \sum_{k=1}^{n} \abs{A\cap B \cap \Set{2k-1,2k}} = \sum_{k=1}^{n} c_k
    \]
    where
    \[
        c_k=\begin{cases}
            0 & b_k=2k-1 \\
            1 & b_k=2k
        \end{cases}
    \]
    It means for any series $\Set{x_i}, x_i\in\Set{0,1}$, there exists $B$ with asymptotic density $\flatfrac{1}{2}$ such that $c_i=x_i$.
    Hence let
    \[
        c_i = \begin{cases}
            0 & i=1                         \\
            1 & i = [3^{2k}+1  ,3^{2k+1}]   \\
            0 & i = [3^{2k+1}+1  ,3^{2k+2}]
        \end{cases}
        \qquad        k\in\NN
    \]


    Then for $n=3^{2k+1}$, $S_n \ge \sum_{3^{2k}+1}^{3^{2k+1}} c_i = 2\times 3^{2k}= \frac{2}{3}n$  and for $n=3^{2k+2}$, $S_n= S_{3^{2k+1}}\le 3^{2k+1}= \frac{1}{3}n$.
    So $L_n:=\flatfrac{ \abs{A\cap B \cap \Set{1,2,\dots,n}}}{n}\ge \flatfrac{1}{3}$ when $n=2\times3^{2k+1}$ and $L_n\le \frac{1}{6}$ when
    $n=2\times 3^{2k+2}$, whence $L_n$ diverges and then $\mathcal{A}$ is not an algebra.

    \textit{(The motivation here is for $0<\flatfrac{S_n}{n}$<1 we could add a lot of 1's to making $\flatfrac{S_{n+m}}{n+m}>1-\epsilon$ for any epsilon;
        and a lot of 0's to making $\flatfrac{S_{n+m}}{n+m}<\epsilon$ for any $\epsilon>0$. Thus the quotinet could fluctuates and then not converge.)}
\end{solution}

\section{Distributions}
\begin{exercise}
    Suppose $X$ and $Y$ are random variables on $(\Omega,\mathcal{F}, P)$ and let $A \in \mathcal{F}$.
    Show that if we let $Z(\omega) = X(\omega)$ for $\omega \in A$ and $Z(\omega) = Y (\omega)$ for
    $ \omega\in A^c$, then Z is a random variable.
\end{exercise}
\begin{solution}
    $Z = X\cdot1_A+Y\cdot1_{A^c}$ is a random variable.
\end{solution}

\begin{exercise}
    Let $\chi$ have the standard normal distribution. Use Theorem 1.2.6 to get upper and lower bounds on $P(\chi\le 4)$.
\end{exercise}
\begin{solution}
    Theorem 1.2.6 is listed at \cref{thm: original 1.2.6}.
    \[
        P(\chi\le 4) = \int_{-\infty}^{4} e^{\flatfrac{x^2}{2}}\dif x = 1 - \int_{x}^{+\infty} e^{\flatfrac{x^2}{2}}\dif x
    \]
    Applying the theorem gives
    \[
        (\frac{1}{4}-\frac{1}{64})e^{-8} \le \int_{x}^{+\infty} e^{\flatfrac{x^2}{2}}\dif x \le \frac{1}{4}e^{-8}
    \]
    So
    \[
        1-\frac{1}{4}e^{-8} \le P(\chi\le 4) \le 1-\frac{15e^{-8}}{64}
    \]
\end{solution}
\begin{exercise}
    Show that a distribution function has at most countably many discontinuities.
\end{exercise}
\begin{solution}
    This is a common conlusion in real analysis. Let $f$ be a distribution function, then $f$ is monotonic increasing.
    Let $P$ be the set of discontinuities.
    For each $x_0\in P$, $f_-(x_0)$, $f_+(x_0)$ exist and $f_-(x_0)$ < $f_-(x_0)$.
    Then letting
    \[
        \mathcal{I}:= \Dset{\left(f_-(a), f_+(a)\right)}{a\in P}
    \]
    and
    \begin{align*}
        \varphi: & P\to \mathcal{I}                  \\
                 & x_0 \mapsto (f_-(x_0) , f_-(x_0))
    \end{align*}

    intervals in $\mathcal{I}$ are disjoint and $\varphi$ is bijective and in since for any $x<y$,
    \[
        f_-(x) < f_+(x)\le f_-(y) < f_+(y)
    \]
    The desired result follows from $\mathcal{I}$, a collection of disjoint open intervals in $\RR$, is at most countable since $\exists q_{a,b}\in \QQ$, $q_{a,b}\in(a,b)\in\mathcal{I}$,
    and then $(a,b)\mapsto q_{a,b}$ is an injection to $\QQ$.
\end{solution}

\begin{exercise}
    Show that if $F(x) = P(X \le x)$ is continuous then $Y = F(X)$ has
    a uniform distribution on $(0,1)$, that is, if $y \in [0, 1]$, $P(Y \le y) = y$.
\end{exercise}
\begin{proof}
    We have $0 \ge F(x)\le 1$, so $P(Y\le y)=0$ for $y<0$ and $P(Y\le y)=1$ for $y>1$. For $y\in [0,1]$,
    \[
        P(Y\le y) = P(F(X) \le y) = P(X\in \Dset{h}{F(h)\le y})
    \]
    Let $h_0 = \sup \Dset{h}{F(h)\le y}$. Since $F$ is increasing and continuous,  $F(h_0) = y$. Therefore
    \[
        P(Y\le y) = P(X\le h_0) = F(h_0)=y
    \]
\end{proof}
\begin{exercise}
    \label{exc: 1.2.5}
    Suppose $X$ has continuous density $f$, $P( \alpha \le  X\le \beta) = 1$ and
    g is a function that is strictly increasing and differentiable on $(\alpha,\beta)$.
    Then $g(X)$ has density
    \[
        \rho (y)=\begin{cases}

            \frac{f(g^{-1}(y))}{g'(g^{-1}(y))} & y \in (g(\alpha), g(\beta)) \\
            0                                  & otherwise
        \end{cases}
    \]
    When $g(x) = ax + b$ with $a > 0$, $g^{-1}(y) = \frac{y - b}{a}$ so the answer is $\frac{1}{a}f(\frac{y - b}{a}).$
\end{exercise}
\begin{proof}
    Since $g$ is strictly increasing, $g$ is invertible and
    therefore
    \[
        P(g(X)\le y) = P(X\le g^{-1}(y))=
        \begin{cases}
            0                                    & y\in (-\infty, g(\alpha)] \\
            \int_{\alpha}^{g^{-1}(y)} f(x)\dif x & y\in(g(\alpha), g(\beta)) \\
            1                                    & y\in [g(\beta), +\infty)
        \end{cases}
    \]
    We could direct differentiate the right or replace $x=g^{-1}(u)$, and then
    \[
        \int_{\alpha}^{g^{-1}(y)} f(x)\dif x = \int_{g(\alpha)}^{y} f(g^{-1}(u))\dif g^{-1}(u) = \int_{g(\alpha)}^{y} \frac{f(g^{-1}(u))}{g'(g^{-1}(u))} \dif u
    \]
\end{proof}
\begin{exercise}
    Suppose $X$ has a normal distribution. Use the previous exercise to compute the density of $\exp(X)$.
    (The answer is called the lognormal distribution.)
\end{exercise}
\begin{proof}
    By \cref{exc: 1.2.5}, letting $f(x)=\frac{1}{\sqrt{2\pi}}e^{\flatfrac{-x^2}{2}}$, the distribution is
    \[
        g(x) = \begin{cases}
            \frac{f(\ln x)}{\exp(\ln(x))} = \frac{1}{x\sqrt{2\pi}}e^{-\frac{(\ln x)^2}{2}} & x\in (0,+\infty)  \\
            0                                                                              & x\in (-\infty, 0]
        \end{cases}
    \]
\end{proof}

\begin{exercise}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item Suppose $X$ has density function $f$. Compute the distribution function of $X^2$ and then differentiate to find its density function.
        \item Work out the answer when X has a standard normal distribution to find the density of the chi-square distribution.
    \end{enumerate}
\end{exercise}
\begin{proof}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item Since
              \[
                  P(X^2\le y) =
                  \begin{cases}
                      0                                        & y\in (-\infty, 0] \\
                      P\left(-\sqrt{y}\le X\le \sqrt{y}\right) & y\in (0,+\infty)
                  \end{cases}
              \]
              and replacing $x=\sqrt{u}$,
              \begin{align*}
                  P\left(-\sqrt{y}\le X\le \sqrt{y}\right) & = \int_{-\sqrt{y}}^{\sqrt{y}} f(x)\dif x = \int_{0}^{\sqrt{y}} (f(x)+f(-x))\dif x                                     \\
                                                           & =\int_0^{y} (f(\sqrt{u})+ f(-\sqrt{u})) \dif \sqrt{u} = \int_0^{y} \frac{f(\sqrt{u})+ f(-\sqrt{u})}{2\sqrt{u}} \dif u
              \end{align*}
              $X^2$ has a density function,
              \[
                  g(y)=\begin{cases}
                      0                                           & y\in (-\infty, 0] \\
                      \frac{f(\sqrt{y})+ f(-\sqrt{y})}{2\sqrt{y}} & y\in (0,+\infty)
                  \end{cases}
              \]

              (\textit{We can not directly differentiate the definite intergrals with variable limits, since it may be indifferentiable when $f$ is discontinuous})
        \item Applying above conclusion, the density is
              \[
                  g(y)=\begin{cases}
                      0                                          & y\in (-\infty, 0] \\
                      \frac{e^{\flatfrac{-y}{2}}}{\sqrt{2\pi y}} & y\in (0,+\infty)
                  \end{cases}
              \]
    \end{enumerate}
\end{proof}
\section{Random Variables}
\begin{exercise}
    Show that if $\mathcal{A}$ generates $\mathcal{S}$,
    then $X^{-1}(\mathcal{A})  \equiv \Set{\Set{X \in A} : A \in \mathcal{A}}$ generates   $\sigma(X) = \Set{\Set{X \in B} : B \in \mathcal{S}}$.
\end{exercise}
\begin{proof}
    It is equivalent to prove $\sigma(X^{-1}(\mathcal{A}))= \sigma(X)$. Since $X^{-1}(\mathcal{A})\subset \sigma(X)$ is obvious and hence $\sigma(X^{-1}(\mathcal{A}))\subset \sigma(X)$,
    it suffices to prove  $\sigma(X^{-1}(\mathcal{A}))\subset \sigma(X)$.
    Considering
    \[
        \mathcal{E}:= \Dset{B\in \mathcal{S}}{X^{-1}(B)\in \sigma(X^{-1}(\mathcal{A}))}
    \]
    it is not diffcult to see $\mathcal{E}$ is $\sigma$-algebra containing $\mathcal{A}$, whence $\mathcal{S}\subset\mathcal{E}$ and then $\sigma(X^{-1}(\mathcal{A}))\subset \sigma(X)$.

    A more common statement is
    \[
        \sigma(X^{-1}(\mathcal{A})) = X^{-1}(\sigma(\mathcal{A}))
    \]
\end{proof}

\begin{exercise}
    Prove Theorem 1.3.6 when $n = 2$ by checking $\Set{X_1+X_2 < x} \in \mathcal{F}$.
\end{exercise}
\begin{proof}
    Theorem 1.3.6 is listed at \cref{thm: original 1.3.6}.
    We observe that
    \[
        \Set{X_1+X_2 < x} = \bigcup_{q\in \QQ} \Set{X_1<q}\cap\Set{X_2<x-q}
    \],
    since the left set contains the right set is obvious and for each $(x_1, x_2)\in  \Set{X_1+X_2 < x}$ , there is $x_1<x-x_2$ and $q_0\in\QQ$,
    $x_1<q_0<x-x_2$, whence $(x_1, x_2)\in\Set{X_1<q_0}\cap\Set{X_2<x-q_0}$ and the left set is contained in the right set.

    For the complete proof of $n=k$, we could use the induction on $n$ and the fact that $X_1+\dots+X_k= (X_1+\dots+X_{k-1})+X_k$.

\end{proof}

\begin{exercise}
    Show that if $f$ is continuous and $X_n \to X$ almost surely then $f(X_n) \to f(X)$ almost surely.
\end{exercise}
\begin{proof}
    For $\omega\in \Set{X_n\to X}$, $f((X_n)(\omega)) \to f(X(\omega))$. Therefore, $\Set{X_n \to X} \subset \Set{f(X_n) \to f(X)}$
    and hence
    \[
        P\left( \Set{f(X_n) \to f(X)}\right) \ge P\left(\Set{X_n \to X}\right) =1
    \]
\end{proof}

\begin{exercise}
    \leavevmode
    \begin{enumerate}[label=(\roman*)]
        \item Show that a continuous function from $\RR^d \to \RR$ is a measurable map from $(\RR^d,\mathcal{R}^d)$ to $(\RR,\mathcal{R})$.
        \item Show that $\mathcal{R}^d$ is the smallest $\sigma$-field that makes all the continuous functions measurable.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Let $f$ be a continuous function from $\RR^d \to \RR$.
    \begin{enumerate}[label=(\roman*)]

        \item  It suffices to prove for all open set $V\subset \RR$, $f^{-1}(V)\in \mathcal{R}^d$. Actually $f^{-1}(V)$ is an open set in $\RR^n$ and hence in $\mathcal{R}^d$, which is a directly result from  general topology and can be proven by the following.

              For each $x\in f^{-1}(V)$,there is $\epsilon_x>0$ such that the open ball $U(f(x), \epsilon_x)\subset V$ and hence there is $\delta_x>0$,
              \[
                  f(U(x, \delta_x))\subset U(f(x), \epsilon_x)\subset V
              \]
              So
              \[
                  f^{-1}(V) = \bigcup_{x\in f^{-1}(V)} U(x, \delta_x)
              \]
              is an open set contiained in $\mathcal{R}^n$.
        \item Let $\mathcal{F}$ be the smallest $\sigma$-field that makes all the continuous functions measurable. For the continuous function,
              \[
                  f_{x_0,d_0}(x)=d_0 - d(x_0, x)
              \]
              there is
              \[
                  \Set{f_{x_0,d_0}>0}=\Set{d(x_0, x)>d_0} = U(x_0, d_0)\in \mathcal{F}
              \]
              So $\mathcal{F}$ contains all open balls and hence contains all open set, which gives $\mathcal{R}^n\subset\mathcal{F}$. The desired result follows.
    \end{enumerate}
\end{proof}

\begin{exercise}
    \label{exc: l.s.c}
    A function $f$ is said to be lower semicontinuous or l.s.c. if
    \[
        \liminf_{y\to x} f(y) \ge f(x)
    \]
    and upper semicontinuous (u.s.c.) if $-f$ is l.s.c.
    Show that $f$ is l.s.c.
    if and only if $\Set{x : f(x) \le a}$ is closed for each $a \in \RR$ and conclude that semicontinuous functions are measurable.
\end{exercise}
\begin{proof}
    Let $S=\Set{x : f(x) \le a}$.
    To prove the forward direction, for $\Set{x_n}\subset S$, $x_n\to x$ implies $f(x)\le \liminf x_n\le a $, whence $x\in S$ and $S$ is closed.
    For the contrary direction, suppose $a=\liminf_{y\to x}f(y)$ and there is a sequence $x_n\to x$, $f(x_n)\to a$.
    Since $S_\epsilon=\Dset{x}{f(x)\le a+\epsilon}$ is closed and $\Set{x_n}_{n\ge m}\subset S_\epsilon$ for a large $m$, $x\in S_\epsilon$.
    So
    \[
        x \in \bigcap_{n=1}^\infty S_{\frac{1}{n}} = \Dset{x}{f(x)\le a}
    \]
    Therefore $f(x)\le a=\liminf_{y\to x}f(y)$ and $f$ is l.s.c.
    Then $f$ is measurable follows from $\Dset{x}{f(x)>a}$ is an open set and then a measurable set.
\end{proof}

\begin{exercise}
    Let $f: \RR^d \to \RR$ be an arbitrary function and let $f^\delta(x) =\sup\Set{f(y) : |y - x| < \delta}$
    and $f_\delta(x) = \inf\Set{f(y) : |y - x| < \delta}$ where
    $\abs{z} = (z^2_1 + \dots + z^2_d)^{\flatfrac{1}{2}}$. Show that $f^\delta$ is l.s.c. and $f_\delta$ is u.s.c.
    Let $f^0 = \lim_{\delta\to 0} f^\delta$, $f_0 = \lim{\delta\to 0} f_\delta$,
    and conclude that the set of points at which $f$ is discontinuous $= \Set{f^0 \neq f_0}$ is measurable.
    % follows from the fact that f0 âˆ’ f0 is.
\end{exercise}
\begin{proof}
    Use the conclusion in \cref{exc: l.s.c} to prove $f^\delta$ is a l.s.c.
    Let $S=\Set{x : f(x) \le a}$ and $\Set{x_n}\subset S$, $x_n\to x$. Suppose $f^\delta(x)>a$ for the sake of contradiction.
    There is $f(y)>a+\epsilon$ where $\abs{y-x}<\delta-\tau$ and $\abs{x_k-x}<\tau$ for a large $k$.
    Thus $\abs{x_k-y}<\delta$ and $f^\delta(x_k)\ge f(y)=a+\epsilon$ leads a contradiction with $x_k\in a$.
    $f^\delta$ is l.s.c follows from $x\in S$ and then $S$ is closed. And $f_\delta$ is a u.s.c since
    \[
        -f_\delta = -\inf\Set{f(y) : |y - x| < \delta} = \sup\Set{-f(y) : |y - x| < \delta} =(-f)^\delta
    \]
    whence $-f_\delta$ is a l.s.c.

    It suffices to prove $f_0(x)=  f(x)\land \liminf\limits_{y\to x} f(y)$ and $f^0(x)= f(x)\lor \limsup\limits_{y\to x} f(y)$.
    And it follows from
    \[
        f_\delta(x)=\inf\Set{f(y) : \abs{y - x} < \delta} = f(x) \land \inf\Set{f(y): 0<\abs{y-x}<\delta} \to f(x)+\liminf_{y\to x} f(y)
    \]
    as $\delta\to 0$ and so does $f^0$.
\end{proof}

\begin{exercise}
    \label{exc: 1.3.7}
    A function $\varphi: \Omega\to \RR$ is said to be simple if
    \[
        \varphi(\omega) = \sum_{m=1}^{n} c_m 1_{A_m}(\omega)
    \]

    where the $c_m$ are real numbers and $A_m \in  \mathcal{F}$.
    Show that the class of $\mathcal{F}$ measurable functions is the smallest class containing the simple functions
    and closed under pointwise limits.
\end{exercise}
\begin{proof}
    By Theorem 1.3.6 in PTE5th(listed at \cref{thm: original 1.3.7}), it is obvious simple functions
    and their pointwise limits are measurable functions.
    It is suffices to prove for each measurable function $f$, there is a sequence of simple functions $\Set{f_n}$, $f_n\to f$.
    Define
    \[
        f_n = \sum_{m=-n}^{n-1} \sum_{k=0}^{n-1} (m+\frac{k}{n}) 1_{A_{m,k}}
    \]
    where $A_{m,k}=f^{-1}([m+\frac{k}{n}, m+\frac{k+1}{n}))$ and then $f_n\to f$.
\end{proof}

\begin{exercise}
    \label{exec: 1.3.8}
    Use the previous exercise to conclude that $Y$ is measurable with respect to $\sigma(X)$ if and only if $Y = f(X)$ where $f: R \to R$ is measurable.
\end{exercise}
\begin{solution}

    It is easy to prove it's sufficient. For the contrary direction, we define $Y_n\to Y$ like $f_n\to f$ in \cref{exc: 1.3.7}.
    Thus
    \[
        Y_n = \sum_{m=-n}^{n-1} \sum_{k=0}^{n-1} (m+\frac{k}{n}) 1_{A_{m,k}^{(n)}}
    \]
    where $A_{m,k}^{(n)}=Y^{-1}([m+\frac{k}{n}, m+\frac{k+1}{n}))\in \sigma(X)$ are disjoint. And since $\sigma(X) = X^{-1}(\mathcal{B})$,
    there must be $C\in \mathcal{B}$ such that $X^{-1}(C)=A_{m,k}^{(n)}$.
    Let
    \[
        B_{m,k}^{(n)}=\bigcap_{C_i\in \mathcal{C}} C_i,\quad \mathcal{C} = \Dset{X\in \mathcal{B}}{X^{-1}(C)=A^{(n)}_{m,k}}
    \]
    and then $B^{(n)}_{m,k}\in \mathcal{B}$, $X^{-1}(B_{m,k}^{(n)})=A_{m,k}^{(n)}$ and $B_{m,k}^{(n)}$ are disjoint for a fixing $n$ and different $m,k$.
    So we define
    \[
        f_n = \sum_{m=-n}^{n-1} \sum_{k=0}^{n-1} (m+\frac{k}{n}) 1_{B_{m,k}^{(n)}}
    \]
    and hence $Y_n=f_n(X)$, $f_n$ is measurable.

    Let $f=\lim_{n\to \infty} f_n$ where $f_n$ is convergent since if $x$ belongs to a certain $B_{m,k}^{(n)}$,
    then $f_n(x)$ is increasing over $n$ otherwise always 0.
    Then $Y=f(X)$ since $Y_n=f_n(X)\to Y=f(x)$ as $n\to\infty$ and $f$ is measurable follows from $f$ is a limit of simple functions.
\end{solution}

\begin{exercise}
    To get a constructive proof of the last result, note that $\Set{\omega:m2^{-n} \le Y < (m + 1)2^{n}} = {X \in  B_{m,n}}$ for some $B_{m,n} \in R$ and
    set $f_n(x) = m2^{-n}$ for $x \in B_{m,n}$ and show that as $n \to \infty$, $fn(x) \to f(x)$.
    and $Y = f(X)$.
\end{exercise}
\begin{solution}
    Just a hint for \cite{exec: 1.3.8}. The specific forms of simple functions approaching to $Y$ is not critical. 
    But it's curious that if the diction here implies there is a simpler and non-constructive proof.
\end{solution}

\appendix
\section{Some related theorem details}

\stepcounter{section}

\subsection*{2\quad Distributions}

\begin{theorem}
    \label{thm: asymptotic expansion of the Gaussian tail probability}
    For $x>0$, leting
    \[
        I(x) := \int_{x}^{+\infty} e^{-\flatfrac{y^2}{2}}
    \]
    then for $n\in \NN$,
    \[
        I(x)= I_n(x) + R_n(x)
    \]
    where
    \begin{align*}
        I_n(x)
        : & = e^{-x^2/2} \left( \frac{1}{x} - \frac{1}{x^3} + \frac{3}{x^5} - \frac{15}{x^7} + \cdots + (-1)^{n-1} \frac{(2n-3)!!}{x^{2n-1}} \right) \\
          & = e^{-x^2/2} \sum_{k=1}^n (-1)^{k-1} \frac{(2k-3)!!}{x^{2k-1}}
    \end{align*}
    and
    \[
        R_n(x) := (-1)^n (2n-1)!! \int_x^{+\infty} \frac{e^{-y^2/2}}{y^{2n}}  \dif y
    \]
    by setting $(-1)!!=1$.
    Then $I_{2k}(x)<I(x) <I_{2k+1}(x)$ for any $k\in\NN$.
\end{theorem}
\begin{proof}
    Use induction on $n$. It's trivial for $n=0$ and assume it holds for $n=k$.
    Since
    \begin{align*}
        \frac{R_k(x)}{(-1)^k (2k-1)!!} & =  \int_x^\infty \frac{e^{-y^2/2}}{y^{2k}}  \dif y  =  \int_x^{+\infty} -\frac{1}{y^{2k+1}}  \dif e^{-y^2/2}       \\
                                       & = \left.\frac{-e^{-y^2/2}}{y^{2k+1}}\right|^{+\infty}_x - \int_x^{+\infty} \frac{(2k+1)e^{-y^2/2}}{y^{2k+2}}\dif y \\\
                                       & = \frac{-e^{-x^2/2}}{x^{2k+1}} -(2k+1) \int_x^{+\infty} \frac{e^{-y^2/2}}{y^{2k+2}}\dif y                          \\
                                       & = \frac{-e^{-x^2/2}}{x^{2k+1}} + \frac{R_{k+1}(x)}{(-1)^k (2k-1)!!}
    \end{align*}

    then
    \begin{align*}
        I & = I_k + R_k = I_k + (-1)^k (2k-1)!!\frac{-e^{-x^2/2}}{x^{2k+1}} + R_{k+1}(x) \\
          & = I_{k+1}(x) + R_{k+1}(x)                                                    \\
    \end{align*}
    The last conclusion is obvious since $R_{2k}>0$ and $R_{2k+1}<0$ for any $k\in \NN$.
\end{proof}

\begin{theorem}[Theorem 1.2.6 in PTE5th]
    \label{thm: original 1.2.6}
    For $x> 0$,
    \[
        (x^{-1}-x^{-3})e^{-\flatfrac{x^2}{2}} \le \int_{x}^{+\infty} e^{-\flatfrac{y^2}{2}} \dif y \le  x^{-1}e^{-\flatfrac{x^2}{2}}
    \]
\end{theorem}
\begin{proof}
    We give a stronger conclusion and a more general proof in  \cref{thm: asymptotic expansion of the Gaussian tail probability}. The desired result directly follows from the cases of $n=1$ and $n=2$.
\end{proof}

\stepcounter{section}
\subsection*{3\quad Random Variables}

\begin{theorem}[Theorem 1.3.6 in PTE5th]
    \label{thm: original 1.3.6}
    If $X_1,\dots ,X_n$ are random variables then $X_1 +\dots+ X_n$ is a random variable.
\end{theorem}
\begin{theorem}[Theorem 1.3.7 in PTE5th]
    \label{thm: original 1.3.7}
    If $X_1,X_2, \dots$ are random variables then so are
    \begin{align*}
         & \inf_n X_n, &  & \sup_n Xn, & \limsup_n Xn, &  & \liminf_n Xn &
    \end{align*}
\end{theorem}
\end{document}